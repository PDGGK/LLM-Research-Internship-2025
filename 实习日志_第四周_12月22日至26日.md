# 🚀 实习日志 | 第四周

> **实习时间**：2025年12月22日（周一）- 12月26日（周五）
> **研究方向**：大规模 Schema 下的 Text-to-SQL 多表关联推理
> **指导方向**：AI 算法实习

---

## 📅 第四周工作概览

### 12月22日 周一 | 列级别 Schema 增强实验

**今日核心成果**

**列级别 Schema 增强实验成功**：使用 Qwen 32B 优化 163 个核心列的描述后，召回率从 39.7% 飙升至 **99.2%**。

---

## 1. 问题诊断

### 1.1 发现根因
之前的列级别检索效果不佳，经分析发现原因：
- 列级别 Schema 只有数据库原始注释（如 `EVENT_TYPE_NAME: 事件类型名称`）
- **表级别有 LLM 优化的描述，列级别没有**
- 列描述太简短，缺乏同义词和业务语境

### 1.2 误区澄清
之前以为需要连接数据库查询真实值才能做列级别匹配，实际上：
- 列级别检索靠的是**列名 + 列描述的语义匹配**
- `sample_rows` 只是辅助，不是核心
- 关键是让列描述包含丰富的业务语境和同义词

---

## 2. 解决方案：LLM 增强列描述

### 2.1 实现方式
使用 Qwen2.5-Coder-32B-Instruct 为核心表的列生成增强描述：

```python
# 生成提示
prompt = f"""
表名: {table_name}
列名: {column_name}
原始描述: {original_desc}
示例值: {sample_rows}

请生成丰富的中文描述（含同义词、业务含义）
"""
```

### 2.2 增强效果示例

| 列 | 原始描述 | 增强描述 |
|----|---------|---------|
| CUSTOMER_NAME | 父ID（错误！） | 客户名称，用于区分不同客户实体。同义词：客户名、用户名称... |
| EVENT_TYPE_NAME | 事件类型名称 | 告警/事件类型，如 Ping event、Trap event。用于分类统计... |

### 2.3 输出
- 处理了 **163 个核心列**（6 张核心表）
- 存储位置：`spider2_dev/schemas_column_enhanced/netcaredb_ai/`

---

## 3. 测试结果

### 3.1 100 问题召回测试（中等 50 + 困难 50）

| 方法 | 平均召回率 | 完全召回 |
|-----|-----------|---------|
| 表级别 Only | 39.7% | 0/100 |
| **表+增强列融合** | **99.2%** | **96/100** |

### 3.2 按难度分布

| 难度 | 表级别 | 表+增强列融合 |
|-----|-------|-------------|
| Medium | 41% (0/50) | **100%** (50/50) |
| Hard | 39% (0/50) | **98%** (46/50) |

### 3.3 关键发现
- **提升 59.5 个百分点！**
- 增强列级别检索变成主要贡献者
- 即使困难问题也能达到 98% 完全召回

---

## 4. 技术细节

### 4.1 当前检索流程

```
问题 → 表级别 Sparse 检索 (TOP 10)
    → 增强列级别 Sparse 检索 (TOP 200 → 聚合到表 TOP 10)
    → 融合（并集）
    → 返回检索到的表
```

### 4.2 性能瓶颈
当前测试 100 个问题需要约 15 分钟，原因：
- 每个问题都要重新计算 schema embedding
- 使用 `compute_score` 而非预计算向量

### 4.3 Embedding 预计算加速 ✅

实现了 embedding 预计算功能：

| 指标 | 之前 | 预计算后 |
|-----|-----|---------|
| 100 问题耗时 | ~15 分钟 | **4.3 秒** |
| 每个问题 | ~9 秒 | **43 毫秒** |
| 加速比 | - | **207x** |

实现位置：`retrieval/precompute_embeddings.py`

预计算文件：`spider2_dev/schema_embeddings.pkl` (0.9 MB)

---

## 5. 深度解析：为什么 Embedding 预计算能加速 207 倍？

### 5.1 先理解 BGE-M3 的 Sparse 检索工作原理

BGE-M3 的 Sparse 检索（词汇权重匹配）本质上是：

```
文本 → Tokenize → Transformer 编码 → 生成每个 token 的权重 → 稀疏向量
```

**核心操作**：把一段文本变成一个**稀疏向量**，向量的每个维度对应词汇表中的一个 token，值为该 token 的重要性权重。

**举例**：
```
"设备告警统计" → {设备: 0.35, 告警: 0.42, 统计: 0.28, ...}
```

### 5.2 之前的方式：为什么慢？

之前使用 `compute_score(sentence_pairs)` 函数：

```
每个问题：
  对于每个 Schema（507 个）：
    1. Tokenize 问题文本          → ~1ms
    2. Tokenize Schema 文本       → ~1ms  
    3. 过 Transformer 编码问题    → ~50ms ← 瓶颈！
    4. 过 Transformer 编码 Schema → ~50ms ← 瓶颈！
    5. 计算词汇匹配得分           → ~0.1ms
                                 ≈ 100ms × 507 = ~50秒
```

**问题所在**：
- 每个问题都要重新编码所有 507 个 Schema
- **Transformer 编码是 O(n²) 复杂度**，非常耗时
- 507 个 Schema × 100ms/个 = **~50 秒/问题**

### 5.3 预计算方式：为什么快？

预计算的核心思想：**Schema 是固定的，不需要每次重新编码！**

```
【预计算阶段（一次性）】
507 个 Schema → Transformer 编码 → 保存 507 个稀疏向量 → 5秒 ← 只做一次！

【检索阶段（每个问题）】
问题 → Transformer 编码（1 次）→ 问题稀疏向量 → ~30ms
问题向量 × 507 个 Schema 向量 → 词汇匹配得分 → ~10ms ← 纯数学运算！
                                          总计 ~40ms
```

**为什么快？**
1. **Schema 编码只做一次**：507 个 Schema 编码一次后保存，不再重复
2. **查询时只编码问题**：每个问题只过一次 Transformer（~30ms）
3. **相似度计算是纯数学**：稀疏向量的点积运算，CPU 毫秒级完成

### 5.4 数学视角：计算量对比

| 操作 | 之前（每问题） | 预计算（每问题） |
|-----|--------------|----------------|
| Transformer 编码 | **508 次** (1 问题 + 507 Schema) | **1 次** (只编码问题) |
| 词汇匹配计算 | 507 次 | 507 次 |

**瓶颈分析**：
- Transformer 编码：~50ms/次，GPU 密集型
- 词汇匹配：~0.02ms/次，纯 CPU 运算

```
之前：508 × 50ms = 25,400ms ≈ 25秒
现在：1 × 50ms + 507 × 0.02ms = 50ms + 10ms = 60ms

加速比：25,400 / 60 ≈ 423 倍（理论值）
实际：207 倍（包含 I/O 开销）
```

### 5.5 类比理解

**菜谱查找的例子**：

假设你是厨师，要回答 100 个"这道菜怎么做"的问题，每次都需要查 500 本菜谱。

**之前的方式（不预计算）**：
```
问题1 → 打开 500 本菜谱 → 逐本阅读 → 找答案 → 关闭所有书
问题2 → 打开 500 本菜谱 → 逐本阅读 → 找答案 → 关闭所有书
...
100 个问题 = 打开/关闭 50,000 次菜谱
```

**预计算方式**：
```
[提前准备] 把 500 本菜谱的目录和关键词抄在一张卡片上
问题1 → 看卡片 → 快速匹配 → 找到答案
问题2 → 看卡片 → 快速匹配 → 找到答案
...
100 个问题 = 看 100 次卡片
```

**卡片 = 预计算的 embedding 向量**
**打开菜谱 = 过 Transformer 编码**

### 5.6 代码实现对比

**之前的慢代码**：
```python
def retrieve(question, schemas):
    pairs = [[question, s["text"]] for s in schemas]  # 507 对
    scores = model.compute_score(pairs)  # 每对都要编码！
    return sorted(scores)
```

**预计算的快代码**：
```python
# 预计算阶段（一次性）
schema_embeddings = model.encode(schema_texts, return_sparse=True)
save(schema_embeddings)

# 检索阶段（每个问题）
def retrieve(question):
    q_vec = model.encode([question], return_sparse=True)  # 只编码问题
    scores = [dot_product(q_vec, s_vec) for s_vec in schema_embeddings]
    return sorted(scores)
```

### 5.7 总结

| 原理 | 解释 |
|-----|-----|
| **为什么能预计算** | Schema 是固定的，embedding 不会变 |
| **为什么快** | 避免重复的 Transformer 编码（最耗时的操作） |
| **加速多少** | 从 508 次编码 → 1 次编码，理论 500+ 倍 |
| **实际加速** | 207 倍（包含 I/O、Python 开销） |
| **适用场景** | 任何 "固定文档 + 动态查询" 的检索场景 |

**核心洞察**：**计算可以换存储**。用 0.9MB 的存储空间，换来 207 倍的速度提升。

---

## 5. 下一步计划

- [ ] 扩展列增强到更多表（当前只有 6 张核心表）
- [ ] 实现 embedding 预计算加速
- [ ] 进行完整的 SQL 生成测试
- [ ] 整理最终报告

---

## 6. 今日总结

今天的工作验证了一个重要假设：**列级别描述的质量直接决定了列级别检索的效果**。通过 LLM 增强列描述，可以显著提升召回率，从根本上解决了之前列级别检索"不work"的问题。

这也说明了：
1. 向量检索的效果很大程度取决于 embedding 文本的质量
2. 简单的数据库注释不够用，需要丰富的业务语境
3. LLM 是生成这类描述的好工具

---

*报告生成时间: 2024-12-22 15:22*

---

### 12月23日 周二 | 全量增强对比实验 & 研究方向反思 ⚠️

**今日核心成果**

**列级别增强策略验证**：通过对比实验证明，核心表增强（163列）远优于全量增强（3353列），全量增强无任何独有优势。

---

#### 1. 全量列增强实验

**实验设计**
- 使用 Qwen 32B 为全部 344 张表的 3353 个列生成增强描述
- 耗时：86.3 分钟（成功 3190 个，失败 0 个）
- 重新预计算 embedding（3697 Schema，5.4 MB）

**实验结果**

| 方案 | 完全召回率 | 平均召回率 |
|-----|-----------|-----------|
| 核心列增强（163列） | **97.4%** | **99.5%** |
| 全量增强（3353列） | 50.3% | 76.6% |

**召回率反而下降 47 个百分点！**

---

#### 2. 根因分析

**按难度分类对比**

| 难度 | 核心列增强 | 全量增强 | 差距 |
|-----|-----------|---------|-----|
| Easy | 100% | 76% | -24% |
| Medium | 100% | 43.6% | -56.4% |
| Hard | 92% | 31.7% | -60.3% |

**表级别 vs 列级别作用分析**

| 检索方式 | 核心列增强 | 全量增强 |
|---------|-----------|---------|
| 表级别 Only | ~40% | 52.3% ✅ 略升 |
| 列级别 Only | ~95% | 48.7% ❌ 大幅下降 |
| 表+列融合 | 97.4% | 50.3% |

**发现**：问题出在列级别检索，全量增强时 95% 的列是噪音。

**失败问题交叉对比**

| 对比类型 | 问题数 |
|---------|-------|
| 核心列成功 + 全量失败 | **142** |
| 全量成功 + 核心列失败 | **0** |

**结论**：全量增强**没有任何独有优势**。

---

#### 3. 技术文档完善

**Embedding 预计算原理详解**

完成了详细的技术文档（约 1100 行），包括：
- 稀疏向量 vs 稠密向量
- Transformer 工作原理
- 注意力机制详解
- Q/K/V 向量计算过程
- Encoder vs Decoder 区别
- 预计算加速原理（207 倍加速）

---

#### 4. 产出文件

| 文件 | 说明 |
|-----|-----|
| `column_enhancement_comparison_report.md` | 核心列 vs 全量列对比报告 |
| `embedding_precomputation_deep_dive.md` | 技术原理详解 |
| `recall_test_300_full_report.md` | 302 问题召回测试报告 |

---

#### 5. 💭 深刻反思：研究方向的根本性错误

> 这是我在 24 号写的对 23 号的反思，与导师讨论后才意识到问题的严重性。

**我们犯了一个根本性的错误：控制变量错了。**

**错误的研究设定**

一开始，我通过观察数据，得出了一个假设：我们的数据库中，有一些表是比较重要的（如设备类型、使用记录、故障记录等）。我想当然地认为，只要增强这些核心表的检索能力，不就能提升整体算法的准确度了吗？

然后我让 DeepSeek 基于这 344 张表生成测试问题，用来测试"是否每次都能检索到这些核心表"。

**这是完全错误的！**

正确的做法应该是：
- **让 DeepSeek 针对 3 张核心表生成问题**
- 然后测试在 344 张表中能否每次都检索到这 3 张表

而不是：
- 让 DeepSeek 看着 344 张表随便生成问题
- 期待检索结果恰好是核心表

**这是典型的"面向结果编程"**

我们被测试集的特殊性误导了：
1. 案例问题和 DeepSeek 生成的问题，恰好很多都涉及核心表
2. 所以当我优化核心表检索后，召回率确实上升了
3. 但这只是**巧合**，不是真正解决了问题

实际情况根本不是这样，甚至可以说**完全不是**。

**一开始就错了，后面全错**

如果我们研究一个课题，问题都没有设立正确，那谈何研究成功？

这次经历让我意识到：
- **问题定义比解决方案更重要**
- 我用了大量的 AI 辅助研究，但 AI 不会帮你纠正研究设定的根本性错误
- **高手还是高手，PhD 还是 PhD**——我觉得自己有点笨了

**24 号的方向调整**

基于这次反思，24 号需要大换方向：
1. **重新设立问题**：明确研究的正确变量控制
2. **重新设计实验**：生成针对特定表的测试问题
3. **避免面向结果编程**：测试集要和优化目标解耦

这是一个很大的教训。

---

*报告生成时间: 2024-12-24*

