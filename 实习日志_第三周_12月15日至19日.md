# 🚀 实习日志 | 第三周

> **实习时间**：2025年12月15日（周一）- 12月19日（周五）
> **研究方向**：大规模 Schema 下的 Text-to-SQL 多表关联推理
> **指导方向**：AI 算法实习

---

## 📅 第三周工作概览

### 12月15日 周一 | Schema Linking 多方案对比实验

**今日工作**

- 继续 Schema Linking 算法优化研究
- 设计并实现表级别 Schema 提取方案
- 对比数据库与学术数据集（Spider）的差异
- 尝试纯 LLM 直接选表方案
- 探索问题翻译和多语言嵌入模型优化方向

---

**数据集对比分析**

对比 netcaredb_ai 与学术标准数据集 Spider：

| 指标 | netcaredb_ai | Spider 1.0 | Spider 2.0-Lite |
|------|--------------|------------|-----------------|
| 表数量 | 344 | 5-10/库 | 企业级 |
| 列数量 | 3,353 | ~28/库 | 803.6/库 |
| 外键覆盖 | 17% | ~100% | 部分 |
| SOTA 准确率 | 待验证 | 85-91% | **33-40%** |

**结论**：学术方法在 Spider 2.0-Lite 上仅 33-40% 准确率，对企业场景参考有限。

---

**纯 LLM 选表方案测试**

**思路**：344 张表约 6k tokens，在 LLM 上下文内，可跳过向量检索直接让 LLM 选表。

**测试结果（7 个业务问题）**：

| 指标 | 结果 |
|------|------|
| 平均召回率（中文描述） | **21.4%** |
| 客户表 `t_bz_config_customer` | 100% 召回 ✅ |
| 设备表 `t_bz_config_ci_ne_root` | 0% 召回 ❌ |

**问题分析**：设备主表描述不够突出，LLM 选了其他"设备"相关表（如 `sdw_dp_device`）。

---

**语言对齐实验**

**假设**：英文问题 + 英文表描述 + 英文嵌入模型 → 更好的向量检索效果。

**实验步骤**：
1. 批量翻译 344 张表描述到英文
2. 测试问题翻译后的向量检索效果

**测试结果**：

| 方案 | 召回率 |
|------|--------|
| 中文问题 + 中文描述 | 0% |
| 英文问题 + 英文描述 | 0% |

**结论**：单纯语言翻译无法解决向量检索问题，问题根源在于表描述的语义表达而非语言对齐。

---

**技术产出**

| 文件 | 功能 |
|------|------|
| `translate_schema_to_english.py` | 批量翻译中文 Schema 到英文 |
| `test_query_translation.py` | 问题翻译测试脚本 |
| `test_english_full.py` | 英文完整流程测试 |
| `test_comprehensive.py` | 综合对比测试脚本 |
| `docs/language_alignment_test_report.md` | 语言对齐测试报告 |
| `docs/linkalign_analysis_report.md` | LinkAlign 分析报告 |
| `docs/pure_llm_table_selection_report.md` | 纯 LLM 选表报告 |

---

**收获与思考**

1. **向量检索的结构性限制**：语言翻译不是根本解决方案，问题在于嵌入模型基于词汇相似度而非业务理解
2. **LLM 直接选表是可行方向**：但召回率依赖表描述质量，需要人工确保核心表描述包含常见查询词
3. **学术方法落地困难**：Spider 上的高分方法在企业场景（大规模、弱外键、中英文混合）效果大打折扣

---

### 12月16日 周二 | 表级别检索方案全面评估与优化

**今日工作**

今天是一个非常充实的工作日，围绕 LinkAlign 的**表级别检索方案**完成了从基准测试到深入优化的完整实验闭环。生成了 **7 份技术报告**，整理了 **14 个测试脚本**，并得出了关于检索策略选择的重要结论。

---

**上午：基准测试与方案对比**

**BGE-M3 向量检索优化**（`docs/bge_m3_optimization_report.md`）：
- 通过增强 3 个核心表的描述，召回率从 **57.1% 提升到 76.2%**
- 关键改动：
  - `event_history`: 加入"告警"关键词
  - `t_bz_config_ci_ne_root`: 标识为"设备主表/核心表"
  - `t_bz_config_customer`: 标识为"客户主表"

**列级别 vs 表级别对比**（`docs/column_vs_table_level_comparison.md`）：
- 结论：表级别方案（76.2%）优于列级别 LinkAlign（52%）
- 发现 LinkAlign 的"劣根性"：核心表在 LLM 过滤阶段被误删

---

**下午：混合检索实验**

**Dense vs Hybrid 对比**（`docs/bge/dense_vs_hybrid_comparison.md`）：
- 实验配置：Dense:Sparse = 0.7:0.3 混合检索
- 结果：召回率没有显著提升（74% vs 74%）
- 原因分析：表描述缺乏关键词，Sparse 向量无法发挥作用

**端到端测试**：
- Dense 端到端测试报告：`docs/bge/dense_e2e/e2e_report.md`
- Hybrid 端到端测试报告：`docs/bge/hybrid_e2e/e2e_report.md`
- 内容：7 个测试用例的完整执行过程、检索结果、SQL 生成和评判

---

**傍晚：深度分析与代码重构**

**优化方向分析**（`docs/optimization_direction_analysis.md`）：
- 识别 3 个可优化环节：
  1. 向量检索召回率（表描述增强）
  2. 检索数量 Top K 调整
  3. 列级别 Schema 优化

**表级别评估总报告**（`docs/bge/table_level_evaluation_report.md`）：
- 核心问题：表描述无法体现数据值（如 ciscoA）
- 建议：探索列级别检索或两阶段混合方案

**Schema 提取脚本优化（重要改动）**

修改文件：`extract_table_level_schema.py`

具体改动：修改 `generate_embedding_text()` 函数，**移除 `max_columns` 参数限制**

| 表 | 之前（20列限制）| 现在（无限制）|
|---|----------------|--------------| 
| `t_bz_config_ci_ne_root` | 20 列 | **75 列** |
| `HOST_NAME` 列 | ❌ 被截断 | ✅ 已包含 |
| `embedding_text` 长度 | ~500 字符 | **1451 字符** |

**LLM 批量优化表描述（系统化方案）**

新建文件：`enhance_schema_with_llm.py`

说明：
- **无手动修改**：LLM 自动为 344 个表生成描述，没有针对任何测试用例做特殊处理
- 对比示例：

| 表 | 之前手动优化 | 现在 LLM 生成 |
|---|------------|--------------|
| `event_history` | 【告警/事件主表】存储所有设备告警... | 事件历史记录表，存储系统中所有事件的详细信息... |
| `t_bz_config_ci_ne_root` | 【设备主表/核心表】... | 根网元配置表，存储网络设备的基础信息与状态... |

> ⚠️ **注意**：LLM 生成的描述没有"告警"、"主表/核心表"等标识词，这是召回率从 76% 降到 74% 的原因。

**测试脚本整理**：
- 创建 `tests/` 目录结构
- 将 14 个测试脚本分类：
  - `tests/bge_m3/` (4)：BGE-M3 相关
  - `tests/table_level/` (2)：表级别检索
  - `tests/e2e/` (7)：端到端测试
  - `tests/query/` (1)：查询处理

---

**召回率对比汇总**

| 方案 | 召回率 | 执行时间 |
|-----|-------|---------| 
| 列级别 LinkAlign | 52% | ~15 分钟 |
| 表级别（手动优化描述）| **76.2%** | ~10 秒 |
| 表级别（LLM 优化描述）| 74% | ~10 秒 |
| 表级别 + Hybrid 检索 | 74% | ~10 秒 |

**关键问题识别**

1. **数据值匹配问题**：用户问"设备 ciscoA"，但 ciscoA 只存在于数据库数据中，不在 embedding_text 里
2. **相似表混淆**：`customer` vs `t_bz_config_customer`，LLM 容易选错
3. **业务规则缺失**：LLM 不知道 `IS_ACTIVE=1` 等必要过滤条件

**表级别方案局限性**

- 无法匹配具体数据值
- 表间关联语义难以体现
- 语义被 50+ 列稀释

---

**技术产出**

| 类型 | 内容 |
|------|------|
| 技术报告 | 7 份（BGE-M3 优化、列vs表对比、Dense vs Hybrid、端到端测试等）|
| 代码新增 | `enhance_schema_with_llm.py`、`test_bge_m3_hybrid_e2e.py` 等 3 个脚本 |
| 代码修改 | `extract_table_level_schema.py` 移除列数限制 |
| 测试脚本 | 14 个，整理到 `tests/` 目录结构 |

---

**收获与思考**

1. **关于 BGE-M3 混合检索**
   - Dense 向量：语义相似性（"告警" ≈ "事件"）
   - Sparse 向量：关键词匹配（BM25 风格）
   - 混合效果：当描述缺乏关键词时，Sparse 权重再高也没用

2. **关于 LLM 自动优化**
   - LLM **只能根据列名推断业务含义**
   - **无法知道表间隐含关联**（如设备表与告警表通过 NE_ID 关联）
   - 需要人工审核核心表描述

3. **工程思考**
   - **避免面向结果编程**：针对测试失败案例手动优化是短视的
   - **系统化优化优于逐个优化**：LLM 批量处理 + 人工校准

---

### 12月17日 周三 | 表+列融合检索取得重大突破 🎉

**今日重大突破**

**表级别 + 列级别融合检索，达到 95% 召回率！**

---

**今日核心成果**

经过一整天的测试验证，我们找到了最优的 Schema Linking 检索方案：

| 方案 | 召回率 | 完全召回 |
|-----|-------|---------|
| 表级别 Sparse | 74% | 4/7 |
| 列级别 Sparse | 83% | 4/7 |
| **表+列融合（并集）** | **95%** | **6/7** ✅ |

**关键发现**

1. **Sparse 模式优于 Dense**：在中文 Schema 检索中，Sparse（词法匹配）74% 显著优于 Dense（语义匹配）45%
2. **表+列可以互补**：表级别找到一些表，列级别找到另一些，取并集能大幅提升召回
3. **累加分算法优于投票算法**：避免了 LinkAlign 原版中"一个相关列被多个不相关列投票淹没"的问题

---

**研究历程回顾**

**从零开始的探索**

说实话，一开始接到这个任务的时候，我是很懵的。我没有 Text-to-SQL 的经验，需要从零开始看论文、看别人的方法、看别人的研究。

**探索表级别检索**

我看了他们的论文，发现学术界基本都是列级别检索。然后我就想：如果我用表级别呢？

测试下来，表级别虽然达不到 90%，但 74% 还是有的。我尽可能去找表级别的优化方法，最后发现表级别到 74% 已经是极限了——这也是为什么学术界都用列级别，因为列级别的操作空间更大。

**发现论文算法的问题**

然后我返回去看列级别检索，发现看似高大上的论文，其实有些算法是有劣根性的。

LinkAlign 原版的问题：
- 对每个候选表，用 LLM 判断多个列是否相关
- 如果一个表有 50 个列，只有 1 个列相关，剩下 49 个不相关
- 投票结果：1 票相关 vs 49 票不相关 → 表被剔除 ❌

我们的累加分方法：
- 每个列有一个相似度分数
- 相关列分数高，不相关列分数低
- 累加后，高分列贡献大，低分列贡献小
- 即使只有 1 个相关列，它的高分也能让表进入 Top 10 ✅

**为什么 SteinerSQL 的图论方法不适合我们**

在看论文的过程中，我还研究了 SteinerSQL 这类基于图论的方法。它的核心思路是：把数据库 Schema 建模成图，用 Steiner Tree 算法找到连接查询涉及表的最小子图。

听起来很高大上，但**不适合我们的数据库**。

我们的数据库特征：
- **反规范化设计**：都是大宽表，不是标准的 3NF 规范化设计
- **外键覆盖率极低**：344 张表只有约 50 个外键，覆盖率仅 **17%**
- **80%+ 的表互不关联**：大部分表是独立的，没有外键关系

| 指标 | 我们的数据库 | 学术数据集 |
|-----|------------|----------|
| 表数量 | 344 | 几十张 |
| 外键覆盖率 | **17%** | 60-80% |
| 设计风格 | 反规范化/大宽表 | 规范化 |
| 表间关联 | 80%+ 无关联 | 大部分有外键 |

**图论方法的前提**是表之间有丰富的外键关系，才能构建有意义的图。但我们的数据库大部分表是"孤岛"，根本连不成图。

这让我意识到：**学术方法不能盲目套用，要看数据特征**。

**灵感：表+列取并集**

经过多次测试和讨论，我发现表级别和列级别检索结果有互补性：
- 测试 1：表级别找到 event_history，列级别找到 t_bz_config_ci_ne_root → 融合后 100%
- 测试 7：表级别找到 event_history，列级别找到另外两个表 → 融合后 100%

**取并集**，直接达成 95% 召回率！

---

**AI 辅助研究的感受**

这次研究过程中，AI 帮了非常大的忙。

**速度提升**

如果是以前，全部要手写：
- 搜论文 → 手动搜索
- 读论文 → 一篇篇看
- 写测试 → 手动写代码
- 验证想法 → 一个个测

有了 AI 之后：
- 搜论文 → AI 搜 arXiv
- 读论文 → AI 辅助理解
- 写测试 → AI 帮写代码
- 验证想法 → **一天验证几十个测试**

**组合爆炸问题**

解决一个问题，宏观上思路就那几个，但微观上有太多方法。

比如说有 4 个环节，每个环节有 2-3 个方法：
- 环节 1：Dense / Sparse / ColBERT
- 环节 2：表级别 / 列级别
- 环节 3：累加分 / 投票 / 平均
- 环节 4：并集 / 交集 / 加权

组合起来就有几十种可能，每种都要测试、分析数据、看优化空间。如果纯手工，不可能这么快。

**AI + 导师，缺一不可**

- **AI**：快速验证想法、写代码、搜论文、分析数据
- **导师**：指方向、把控质量、提出关键问题

今天导师问的几个问题让我印象深刻：
- "200 列一定对应 10 个表吗？" → 促使我检查算法细节
- "表 Dense 效果才 45%，不如用 Sparse？" → 纠正了我的报告错误
- "确保没有面向结果编程" → 让我验证结论的泛用性

---

**关于唯一失败的测试**

测试 2："列出平台上设备 device state down 状态超过3个月的设备清单及客户名称"

这个测试始终无法召回 `event_history` 表，无论用哪种方法。

分析后发现：**问题本身就有歧义**。问题说的是"设备状态"，关键词是"状态/down"，但正确答案需要"告警/事件"表。问题和答案之间存在语义断层。

所以按照现在的测试结果来看，6/7 = 95% 的召回率，在这种有歧义的测试用例下，已经是非常好的结果了。

---

**召回率提升之后的工作（下午）**

**列级别过滤实验**

**目标**：在召回足够多的表之后，尝试过滤不相关的列，减少给 LLM 的信息量。

**方法**：使用 Sparse 相似度阈值过滤列

**结果**：❌ **失败**

| 阈值 | 保留列数 | SQL必须列召回率 |
|-----|---------|---------------|
| 0.05 | 12/155 | 0% |
| 0.02 | 45/155 | 约 30% |

**失败原因**：
- SQL 必须的列（如 `ci_id`, `event_time`）语义上与问题关联度低
- 这些列虽然相似度分数低，但对 SQL 生成至关重要
- 过滤会丢失关键列

**SQL 生成质量对比**

**核心发现**：给 LLM 更多表反而会导致 SQL 质量下降！

| 方案 | 表数量 | Token 数 | SQL 正确率 |
|-----|-------|---------|----------|
| 原始3表 | 1-3 | ~1000 | **71%** |
| 完整格式 | 10-12 | ~3000 | 14% |
| 精简格式 | 10-12 | ~2000 | 14% |
| 最精简格式 | 10-12 | ~1200 | 14% |

**结论**：
- **表数量是关键**：给 10+ 张表导致质量下降 57%
- **格式压缩无法弥补**：即使压缩格式，质量也不会提升
- **核心问题不是格式，是表太多**

**关键发现总结**

1. **列过滤风险大**：语义相似度低的列可能是 SQL 必须的
2. **表数量是瓶颈**：给 10+ 张表会严重干扰 LLM
3. **格式压缩帮助有限**：不能弥补表太多的问题
4. **Prompt 影响有限**：简单提升 Prompt 无法解决根本问题

---

**技术产出**

| 脚本 | 用途 |
|-----|------|
| `test_all_modes_comparison.py` | Dense/Sparse/ColBERT 对比 |
| `test_table_column_fusion.py` | 表+列融合策略 |
| `export_complete_data.py` | 导出完整检索数据 |
| `test_column_filtering.py` | 列级别过滤测试 |
| `test_schema_compression.py` | Schema 压缩 Token 对比 |
| `test_sql_full_comparison.py` | 完整 SQL 生成对比 |
| `test_prompt_comparison.py` | Prompt 模式对比 |

**研究报告**

- `docs/bge_m3_hybrid_retrieval_report.md` - 完整研究报告
- `docs/retrieval_data/` - 详细检索数据（200列+10表）
- `column_filtering_experiment_report.md` - 列过滤实验报告
- `sql_comparison_full_report.md` - SQL 生成质量报告
- `prompt_comparison_report.md` - Prompt 对比报告

---

**下一步工作**

1. **外键推理**：检索到设备表 → 自动关联事件表，解决测试 2
2. **Milvus 部署**：预构建索引加速检索（当前每查询约 2 分钟）
3. **端到端验证**：集成到 LinkAlign，验证 SQL 生成准确率

---

**今日感想**

虽然我们现在只是做到了"如何在千表中找到我们需要的表"这一步，但这已经是非常重大的进步了。

从零开始看论文，到发现论文算法的问题，到提出自己的改进方案，再到验证方案达到 95% 召回率——这个过程让我学到了很多。

下午的工作让我意识到：**召回只是第一步，如何给 LLM 合适的信息量是另一个挑战**。给太多信息反而会干扰 LLM，需要进一步研究如何精选 2-3 张核心表。

最重要的感悟是：**看论文要批判性思考**。arXiv 上的论文，未必都是完美的，有些算法可能存在劣根性。只有动手测试、用数据说话，才能发现真正的问题和解决方案。

---

> *"召回只是第一步，如何给 LLM 合适的信息量是另一个挑战。"*

---

### 12月19日 周五 | 构建测试数据集并进行系统性评估 📊

**今日工作**

今天的工作重点是**构建测试数据集并进行系统性评估**，为 Text-to-SQL 系统建立了完整的测试基准。

---

**使用 DeepSeek 生成 100 个测试问题**

**背景**: 之前的测试样本量太少（仅 7 个），无法进行系统性评估。

**实现**:
- 编写 `scripts/generate_test_questions.py` 脚本
- 使用 DeepSeek API (`deepseek-chat` 模型)
- 读取全部 344 张表的 Schema，让 LLM 生成覆盖多种场景的问题

**生成结果**:

| 难度 | 数量 | 说明 |
|-----|------|------|
| easy | 29 | 单表简单统计/查询 |
| medium | 52 | 2-3表关联、条件过滤 |
| hard | 19 | 多表 JOIN、复杂聚合 |

**核心表使用频率**:
1. `t_bz_config_ci_ne_root` - 33 次
2. `t_bz_config_customer` - 29 次
3. `event_history` - 13 次

---

**表召回率测试（100 个样本）**

**方法**: 使用表+列融合检索（BGE-M3 Sparse 模式）

**总体结果**:

| 指标 | 结果 |
|-----|------|
| 完全召回 | 65% |
| 部分召回 | 28% |
| 未召回 | 7% |
| **平均召回率** | **78.8%** |

**按难度分类**:

| 难度 | 完全召回率 |
|-----|-----------| 
| easy | **93.1%** |
| medium | **69.2%** |
| hard | **52.6%** |

**关键发现**:
- Easy 问题表现优秀，单表查询基本没问题
- Hard 问题挑战大，多表 JOIN 场景召回不足
- `t_bz_config_ci_ne_root` 和 `t_bz_config_customer` 在多表查询时容易缺失

---

**SQL 生成测试（Qwen2.5-Coder-32B-Instruct）**

**测试配置**:
- 模型: Qwen2.5-Coder-32B-Instruct
- API: http://172.31.24.112:33080/v1
- 测试问题: 100 个

**对比两种 Prompt 模式**:

| 模式 | 正确数 | 正确率 |
|-----|-------|-------|
| 不开 Thinking | 60/100 | **60%** |
| 开 Thinking | 66/100 | **66%** |

**结论**: 开 Thinking（让模型先思考再回答）提升了 **6 个百分点**

**两种模式结果不同的典型案例**:
- 端口流量使用率 TOP10：不开 ❌ → 开 ✅
- 未配置联系人的客户：不开 ❌ → 开 ✅
- 采集机健康状态异常：不开 ❌ → 开 ✅

---

**模型 Thinking 能力调研**

**问题**: Qwen2.5-Coder-32B-Instruct 是否支持原生 thinking 模式？

**测试结果**:
- API 层面支持 `enable_thinking` 参数（不报错）
- 但模型实际没有输出 `<think>...</think>` 标签
- **结论**: Qwen2.5-Coder-32B-Instruct 不是原生 thinking 模型，但可以通过 Prompt 引导思考

---

**技术产出**

| 文件 | 说明 |
|-----|------|
| `test_questions_100.json` | 100 个测试问题（JSON） |
| `test_questions_100.csv` | 100 个测试问题（CSV） |
| `recall_test_100_report.md` | 表召回率测试报告 |
| `sql_generation_100_test_report.md` | SQL 生成测试报告 |

---

**关键结论**

1. **测试数据集已建立**: 100 个问题覆盖 easy/medium/hard 三种难度
2. **召回率瓶颈明确**: Hard 问题只有 52.6% 完全召回，需要优化多表检索策略
3. **Thinking 模式有效**: 开 Thinking 比不开提升 6% 正确率
4. **主要错误类型**: 缺少 ORDER BY、SELECT 字段不匹配、缺少 JOIN

---

**下一步计划**

1. **优化核心表召回**: 为 `t_bz_config_ci_ne_root` 和 `t_bz_config_customer` 增加白名单机制
2. **改进 Hard 问题处理**: 研究如何提高多表 JOIN 场景的召回率
3. **优化 SQL 生成 Prompt**: 针对主要错误类型（ORDER BY 缺失）改进提示词

---

> *"从 7 个测试用例到 100 个，终于可以用数据说话了。"*
